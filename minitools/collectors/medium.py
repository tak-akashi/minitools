"""
Medium Daily Digest collector module.
"""

import os
import pickle
import base64
import re
import asyncio
import random
import aiohttp
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
import pytz
from bs4 import BeautifulSoup

from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from minitools.utils.logger import get_logger

logger = get_logger(__name__)

# Gmail API ã‚¹ã‚³ãƒ¼ãƒ—
SCOPES = ["https://www.googleapis.com/auth/gmail.readonly"]

# User-Agent ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒªã‚¹ãƒˆ
USER_AGENTS = [
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:134.0) Gecko/20100101 Firefox/134.0",
]


@dataclass
class Article:
    """è¨˜äº‹æƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    title: str
    url: str
    author: str
    preview: str = ""  # ãƒ¡ãƒ¼ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
    claps: int = 0  # æ‹æ‰‹æ•°
    japanese_title: str = ""
    summary: str = ""
    japanese_summary: str = ""
    date_processed: str = ""


class MediumCollector:
    """Medium Daily Digestãƒ¡ãƒ¼ãƒ«ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, credentials_path: Optional[str] = None):
        self.gmail_service = None
        self.http_session = None
        self.credentials_path = credentials_path or os.getenv(
            "GMAIL_CREDENTIALS_PATH", "credentials.json"
        )
        self._authenticate_gmail()

    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼"""
        connector = aiohttp.TCPConnector(limit=5)  # ä¸¦åˆ—æ•°ã‚’å‰Šæ¸›ï¼ˆbotæ¤œå‡ºå›é¿ï¼‰
        timeout = aiohttp.ClientTimeout(
            total=60, connect=30, sock_connect=30, sock_read=30
        )

        # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æ¨¡å€£ã—ãŸãƒ˜ãƒƒãƒ€ãƒ¼
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9,ja;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0",
        }

        self.http_session = aiohttp.ClientSession(
            connector=connector, timeout=timeout, headers=headers
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.http_session:
            await self.http_session.close()

    def _authenticate_gmail(self):
        """Gmail APIã®èªè¨¼"""
        try:
            creds = None
            token_path = "token.pickle"

            if os.path.exists(token_path):
                with open(token_path, "rb") as token:
                    creds = pickle.load(token)

            if not creds or not creds.valid:
                if creds and creds.expired and creds.refresh_token:
                    creds.refresh(Request())
                else:
                    if not os.path.exists(self.credentials_path):
                        raise FileNotFoundError(
                            f"èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ« {self.credentials_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                        )

                    flow = InstalledAppFlow.from_client_secrets_file(
                        self.credentials_path, SCOPES
                    )
                    creds = flow.run_local_server(port=0)

                with open(token_path, "wb") as token:
                    pickle.dump(creds, token)

            self.gmail_service = build("gmail", "v1", credentials=creds)
            logger.info("Gmail API authenticated successfully")

        except Exception as e:
            logger.error(f"Gmailèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def get_digest_emails(self, date: Optional[datetime] = None) -> List[Dict]:
        """
        Medium Daily Digestãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—

        Args:
            date: å–å¾—ã™ã‚‹æ—¥ä»˜ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ä»Šæ—¥ï¼‰

        Returns:
            ãƒ¡ãƒ¼ãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
        """
        if date is None:
            date = datetime.now()

        # JSTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’è¨­å®š
        jst = pytz.timezone("Asia/Tokyo")

        # dateãŒnaiveã®å ´åˆã¯JSTã¨ã—ã¦æ‰±ã†
        if date.tzinfo is None:
            date = jst.localize(date)

        # æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã®é–‹å§‹ã¨çµ‚äº†ã‚’è¨ˆç®—ï¼ˆJSTï¼‰
        start_date_jst = date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date_jst = start_date_jst + timedelta(days=1)

        # Unix ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›
        start_timestamp = int(start_date_jst.timestamp())
        end_timestamp = int(end_date_jst.timestamp())

        # æ¤œç´¢ã‚¯ã‚¨ãƒª
        query = (
            f"from:noreply@medium.com after:{start_timestamp} before:{end_timestamp}"
        )
        logger.info(f"Searching Gmail with query: {query}")

        try:
            loop = asyncio.get_event_loop()
            if self.gmail_service is None:
                logger.error("Gmail service is not initialized")
                return []

            service = self.gmail_service
            response = await loop.run_in_executor(
                None,
                lambda: service.users()
                .threads()
                .list(userId="me", q=query, maxResults=1)
                .execute(),
            )

            threads = response.get("threads", [])
            if not threads:
                logger.info("No Medium Daily Digest emails found")
                return []

            thread_id = threads[0]["id"]
            thread = await loop.run_in_executor(
                None,
                lambda: service.users()
                .threads()
                .get(userId="me", id=thread_id)
                .execute(),
            )

            messages = thread.get("messages", [])
            if messages:
                logger.info(f"Found {len(messages)} messages in thread")
                return [messages[-1]]  # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’è¿”ã™

            return []

        except HttpError as error:
            logger.error(f"Gmail APIã‚¨ãƒ©ãƒ¼: {error}")
            return []

    def parse_articles(self, html_content: str) -> List[Article]:
        """
        ãƒ¡ãƒ¼ãƒ«HTMLã‹ã‚‰è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º

        Args:
            html_content: ãƒ¡ãƒ¼ãƒ«ã®HTMLå†…å®¹

        Returns:
            è¨˜äº‹æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        soup = BeautifulSoup(html_content, "html.parser")
        articles = []

        # Medium Daily Digestã®è¨˜äº‹ãƒªãƒ³ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
        article_links = soup.find_all(
            "a", class_="ag", href=re.compile(r"https://medium\.com/.*\?source=email")
        )

        seen_urls = set()

        for link in article_links:
            url = str(link.get("href", ""))
            if not url or url in seen_urls:
                continue

            # URLã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆæ”¹å–„ç‰ˆï¼‰
            clean_url = self._clean_url(url)
            if clean_url in seen_urls:
                logger.debug(f"é‡è¤‡URLã‚’ã‚¹ã‚­ãƒƒãƒ—: {clean_url}")
                continue
            seen_urls.add(clean_url)

            # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆh2ã‚¿ã‚°ã‹ã‚‰ï¼‰
            h2_tag = link.find("h2")
            if h2_tag:
                title = h2_tag.get_text(strip=True)
            else:
                title = link.get_text(strip=True)

            if not title or len(title) < 10:
                logger.debug(f"çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {title}")
                continue

            # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡ºï¼ˆh3ã‚¿ã‚°ã‹ã‚‰ï¼‰
            preview = ""
            h3_tag = link.find("h3")
            if h3_tag:
                preview = h3_tag.get_text(strip=True)
                if preview and len(preview) > 20:
                    preview = preview[:500]  # 500æ–‡å­—ã«åˆ¶é™
                else:
                    preview = ""

            # è‘—è€…æƒ…å ±ã®æŠ½å‡ºï¼ˆgreat-grandparentã‹ã‚‰è‘—è€…ãƒªãƒ³ã‚¯ã‚’æ¢ã™ï¼‰
            author = "Unknown"
            parent = link.parent
            grandparent = parent.parent if parent else None
            great_grandparent = grandparent.parent if grandparent else None

            if great_grandparent:
                # è‘—è€…ãƒªãƒ³ã‚¯ã‚’æ¢ã™ï¼ˆmedium.com/@username ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒã¤ã‚‚ã®ï¼‰
                author_links = great_grandparent.find_all(
                    "a", href=re.compile(r"medium\.com/@[^/]+\?")
                )
                for author_link in author_links:
                    if author_link == link:
                        continue
                    author_text = author_link.get_text(strip=True)
                    # æœ‰åŠ¹ãªè‘—è€…å: ç©ºã§ãªãã€ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãªã„
                    if (
                        author_text
                        and len(author_text) > 1
                        and not author_text.startswith("@")
                        and author_text != "Member"
                    ):
                        author = author_text
                        break

            # æ‹æ‰‹æ•°ã®æŠ½å‡ºï¼ˆgreat_grandparent ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ï¼‰
            claps = self._extract_claps(great_grandparent)

            article = Article(
                title=title,
                url=clean_url,
                author=author,
                preview=preview,
                claps=claps,
                date_processed=datetime.now().isoformat(),
            )
            articles.append(article)
            logger.debug(
                f"è¨˜äº‹ã‚’æ¤œå‡º: {title[:50]}... by {author}, claps: {claps}, preview: {len(preview)} chars"
            )

        logger.info(f"Parsed {len(articles)} articles from email")
        return articles

    @staticmethod
    def _parse_count(count_str: str) -> int:
        """
        "1.2K" â†’ 1200ã€"320" â†’ 320 ã®ã‚ˆã†ã«ã‚«ã‚¦ãƒ³ãƒˆæ–‡å­—åˆ—ã‚’æ•´æ•°ã«å¤‰æ›

        Args:
            count_str: ã‚«ã‚¦ãƒ³ãƒˆæ–‡å­—åˆ—

        Returns:
            æ•´æ•°å€¤ï¼ˆãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯0ï¼‰
        """
        if not count_str:
            return 0
        count_str = count_str.strip().upper()
        try:
            if count_str.endswith("K"):
                return int(float(count_str[:-1]) * 1000)
            elif count_str.endswith("M"):
                return int(float(count_str[:-1]) * 1000000)
            else:
                return int(count_str)
        except (ValueError, IndexError):
            return 0

    def _extract_claps(self, container) -> int:
        """
        è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒŠè¦ç´ ã‹ã‚‰æ‹æ‰‹æ•°ã‚’æŠ½å‡º

        Args:
            container: BeautifulSoupã®è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒŠè¦ç´ 

        Returns:
            æ‹æ‰‹æ•°ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0ï¼‰
        """
        if not container:
            return 0

        text = container.get_text(separator=" ", strip=True)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: "Claps" ã®å¾Œã«æ•°å€¤ (ä¾‹: "Claps 320", "Claps320", "Claps 1.2K")
        match = re.search(r"Claps\s*([0-9][0-9.,]*[KkMm]?)", text)
        if match:
            return self._parse_count(match.group(1))

        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ‹æ‰‹ã‚¢ã‚¤ã‚³ãƒ³(ğŸ‘)ã®å¾Œã«æ•°å€¤
        match = re.search(r"ğŸ‘\s*([0-9][0-9.,]*[KkMm]?)", text)
        if match:
            return self._parse_count(match.group(1))

        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: "min read" ã®å¾Œã®æ•°å€¤åˆ—ï¼ˆmin read â†’ claps â†’ responsesï¼‰
        match = re.search(r"min read\s+([0-9][0-9.,]*[KkMm]?)", text)
        if match:
            return self._parse_count(match.group(1))

        return 0

    def _clean_url(self, url: str) -> str:
        """
        URLã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é™¤å»ï¼‰

        Args:
            url: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹URL

        Returns:
            ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸURL
        """
        # URLã‹ã‚‰ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
        clean_url = url.split("?")[0]
        # æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»
        clean_url = clean_url.rstrip("/")
        # Mediumã®ç‰¹æ®Šãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
        if "#" in clean_url:
            clean_url = clean_url.split("#")[0]
        return clean_url

    def _extract_author_from_jina(self, content: str) -> Optional[str]:
        """
        Jina Readerã®å‡ºåŠ›ã‹ã‚‰è‘—è€…åã‚’æŠ½å‡º

        Args:
            content: Jina Readerã‹ã‚‰å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            è‘—è€…åï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneï¼‰
        """
        # ç„¡åŠ¹ãªè‘—è€…åãƒ‘ã‚¿ãƒ¼ãƒ³
        invalid_names = {
            "sitemap",
            "follow",
            "share",
            "menu",
            "home",
            "about",
            "contact",
            "sign in",
            "sign up",
            "login",
            "register",
            "subscribe",
            "newsletter",
            "privacy",
            "terms",
            "help",
            "search",
            "more",
            "read more",
            "continue",
            "medium",
            "member",
            "membership",
            "upgrade",
            "get started",
            "open in app",
            "open app",
            "get the app",
            "download",
            "install",
            "write",
            "read",
            "listen",
            "watch",
            "see more",
            "view more",
            "responses",
            "clap",
            "claps",
            "save",
            "bookmark",
            "copy link",
            "published",
            "edited",
            "updated",
            "posted",
            "featured",
        }

        def clean_author_name(name: str) -> str:
            """è‘—è€…åã‹ã‚‰Markdownæ§‹æ–‡ãªã©ã‚’é™¤å»"""
            if not name:
                return ""
            # Markdownç”»åƒæ§‹æ–‡ã‚’é™¤å»: ![Image N: Author Name -> Author Name
            name = re.sub(r"!\[Image\s*\d*:\s*", "", name)
            # é–‰ã˜æ‹¬å¼§ã‚‚é™¤å»
            name = name.rstrip("]")
            return name.strip()

        def is_valid_author(name: str) -> bool:
            if not name or len(name) < 3 or len(name) > 50:
                return False
            if name.lower() in invalid_names:
                return False
            # è‘—è€…åã¯é€šå¸¸å¤§æ–‡å­—ã§å§‹ã¾ã‚‹å˜èªã‚’å«ã‚€
            if not any(word[0].isupper() for word in name.split() if word):
                return False
            return True

        lines = content.split("\n")

        for i, line in enumerate(lines[:50]):  # æœ€åˆã®50è¡Œã‚’æ¤œç´¢
            line_stripped = line.strip()

            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: "By Author Name" or "by Author Name"
            if line_stripped.lower().startswith("by ") and len(line_stripped) > 3:
                author = line_stripped[3:].strip()
                # "By Author Name in Publication" ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                if " in " in author:
                    author = author.split(" in ")[0].strip()
                author = clean_author_name(author)
                if is_valid_author(author):
                    return author

            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: "Author Name" ã®å¾Œã« "Follow" ãŒã‚ã‚‹è¡Œ
            if i + 1 < len(lines) and "follow" in lines[i + 1].lower():
                if line_stripped and not line_stripped.startswith("#"):
                    author = clean_author_name(line_stripped)
                    if is_valid_author(author):
                        return author

            # ãƒ‘ã‚¿ãƒ¼ãƒ³3: Markdown ãƒªãƒ³ã‚¯å½¢å¼ [Author Name](url)
            if (
                line_stripped.startswith("[")
                and "](https://medium.com/@" in line_stripped
            ):
                match = re.match(r"\[([^\]]+)\]", line_stripped)
                if match:
                    author = clean_author_name(match.group(1))
                    if is_valid_author(author):
                        return author

            # ãƒ‘ã‚¿ãƒ¼ãƒ³4: "Written by Author" å½¢å¼
            if "written by " in line_stripped.lower():
                idx = line_stripped.lower().find("written by ")
                author = clean_author_name(line_stripped[idx + 11 :].strip())
                if is_valid_author(author):
                    return author

            # ãƒ‘ã‚¿ãƒ¼ãƒ³5: "Author Name Â· X min read" å½¢å¼
            if " min read" in line_stripped.lower():
                match = re.match(r"^([A-Z][a-zA-Z\s\.]+?)(?:\s*Â·|\s+\d)", line_stripped)
                if match:
                    author = clean_author_name(match.group(1).strip())
                    if is_valid_author(author):
                        return author

            # ãƒ‘ã‚¿ãƒ¼ãƒ³6: Markdown ãƒªãƒ³ã‚¯å½¢å¼ï¼ˆä¸€èˆ¬çš„ãªURLãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            match = re.match(
                r"^\[([A-Z][a-zA-Z\s\.]+?)\]\(https?://[^\)]+\)$", line_stripped
            )
            if match:
                author = clean_author_name(match.group(1).strip())
                if is_valid_author(author):
                    return author

        return None

    def _extract_author_from_url(self, url: str) -> Optional[str]:
        """
        URLã‹ã‚‰è‘—è€…åï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼‰ã‚’æŠ½å‡º

        Args:
            url: Mediumè¨˜äº‹ã®URL

        Returns:
            è‘—è€…ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneï¼‰
        """
        # medium.com/@username/... ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        match = re.search(r"medium\.com/@([^/]+)", url)
        if match:
            username = match.group(1)
            # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚„ãƒã‚¤ãƒ•ãƒ³ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›ã—ã¦èª­ã¿ã‚„ã™ã
            return f"@{username}"
        return None

    async def fetch_article_content(
        self, url: str, max_retries: int = 3
    ) -> tuple[str, Optional[str]]:
        """
        è¨˜äº‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’Jina AI Readerã§å–å¾—ï¼ˆãƒœãƒƒãƒˆæ¤œå‡ºå›é¿ï¼‰

        Args:
            url: è¨˜äº‹ã®URL
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°

        Returns:
            (è¨˜äº‹å†…å®¹, è‘—è€…å) ã®ã‚¿ãƒ—ãƒ«
        """
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®ãƒ©ãƒ³ãƒ€ãƒ é…å»¶
        delay = random.uniform(1, 3)
        await asyncio.sleep(delay)

        # Jina AI Reader URL
        jina_url = f"https://r.jina.ai/{url}"

        for attempt in range(max_retries):
            try:
                user_agent = random.choice(USER_AGENTS)
                headers = {
                    "User-Agent": user_agent,
                    "Accept": "text/plain",
                }

                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        jina_url, headers=headers, timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        if response.status == 200:
                            text_content = await response.text()

                            # JinaãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸå ´åˆã‚’æ¤œå‡º
                            if (
                                "error 403" in text_content.lower()
                                or "just a moment" in text_content.lower()
                            ):
                                logger.warning(
                                    f"Jina Reader blocked by Medium for {url}"
                                )
                                return "", None

                            # è‘—è€…åã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
                            author = self._extract_author_from_jina(text_content)
                            # æ³¨: URLãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ä½¿ã‚ãªã„ï¼ˆãƒ¡ãƒ¼ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸè‘—è€…åã‚’å„ªå…ˆï¼‰

                            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’3000æ–‡å­—ã«åˆ¶é™
                            text_content = text_content.strip()[:3000]

                            if len(text_content) > 100:
                                logger.debug(
                                    f"Jina Reader: {len(text_content)} chars, author: {author}"
                                )
                                return text_content, author
                            else:
                                raise Exception("Content too short")

                        else:
                            raise Exception(f"HTTP {response.status}")

            except Exception as e:
                wait_time = 2**attempt

                if attempt < max_retries - 1:
                    logger.warning(
                        f"Error fetching {url}, retrying in {wait_time}s... (attempt {attempt + 1}/{max_retries}): {e}"
                    )
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"Error fetching article from {url}: {e}")
                    return "", None

        return "", None

    def extract_email_body(self, message: Dict) -> str:
        """
        ãƒ¡ãƒ¼ãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º

        Args:
            message: Gmail APIã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

        Returns:
            ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã®HTML
        """
        payload = message.get("payload", {})
        body = self._extract_body_from_payload(payload)
        return body

    def _extract_body_from_payload(self, payload: Dict) -> str:
        """ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‹ã‚‰ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆå†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰"""
        body = ""

        if "parts" in payload:
            for part in payload["parts"]:
                if part["mimeType"] == "text/html":
                    data = part["body"]["data"]
                    body = base64.urlsafe_b64decode(data).decode("utf-8")
                    break
                elif "parts" in part:
                    body = self._extract_body_from_payload(part)
                    if body:
                        break
        elif payload.get("body", {}).get("data"):
            body = base64.urlsafe_b64decode(payload["body"]["data"]).decode("utf-8")

        return body
