 Hey there and welcome to this video. In this video I'll be presenting score-based generative models. And before you leave this video thinking I've never heard about this topic, this must be super boring and unimportant, let me stop you there. If you're interested in the topic of diffusion models and might even struggle to fully understand it, this video might be really helpful for you. Score-based generative models are a different formulation of diffusion models and might even be considered the broader family that diffusion models belong to. And while the derivation is motivated differently, both score-based generative models and diffusion models will come out at the same point. And to me this formulation makes so much more sense and is so beautiful. When I did my video about diffusion models, there were many things that I didn't really know where they came from. I was able to follow the derivation but a lot of the steps seemed so unexplained. And then when I learned about score-based generative models, it felt like a big relief as I was understanding the ideas behind the derivation. And understanding the ideas behind the derivation is so important unlike just understanding the derivation itself. So trust me, if you're interested in diffusion models, you might really enjoy this topic and video. The outline of this video will be the following. Most of these things will not make sense for now, but they represent the red thread of this video which we'll work along with. So let's get into the details. Assume that we have a data set and we want to learn the probability distribution of this data set such that we can sample from it to generate new data points. We can try to estimate the probability density function or PDF of the data. A PDF is a function that is non-negative and sums to 1 when integrated. This here is the well-known normal distribution and this here is its PDF. A PDF is all you need to sample from a distribution. So if we have the underlying PDF of our data set, we can sample from it and generate new data points. Let's call the PDF of our data set P . It is unknown and we want to approximate it by something we can call P . A good way to parameterize it is like the following. The reason for that is that a negative exponent enforces everything to be non-negative and the learnable constant Z is used for normalization such that the PDF integrates to 1. And if you remember, those are the two constraints a PDF has. You can see the same things in the PDF of the normal distribution. This is the normalizing constant and this negative exponent enforces the outputs to be non-negative. And f theta of x could be learned to output how dense the probability is around x. A high value would mean that we are in a dense area of our data and a low value means we are further away. Trying to actually train this would make us run into a problem. Calculating Z theta requires us to integrate over the entire space of x. This can be extremely high dimensional and intractable to compute. Going forward with this approach requires restrictions to the model or approximating Z theta. But there is a trick that we can apply and we call it the score. The score refers to the log gradient of the probability density function with respect to x. This doesn't seem to make things easier and rather looks even more complicated. But let's take a closer look. First we can apply the logarithm rule and rewrite the division as a subtraction. And now we see that the first logarithm cancels with the exponent. And now look at this. The second term becomes zero because it does not depend on x. It's like having the function f of x is equal to x and taking the derivative with respect to y. As y doesn't occur in f of x, the change of it has no effect on f of x. And therefore the derivative is zero. And now we're just left with the negative gradient with respect to x of f theta of x. This is really good because the normalizing constant falls away which introduced many restrictions to the model. In order to stick with the paper's notation we'll define this to be equal to s theta of x. Now score matching means to approximate the scores of the original distribution with the predicted scores from s theta of x. Like so. This basically means that we try to minimize the difference between the original and predicted scores. Very simple right? And intuitively the score function makes a lot of sense too. It tells you in which direction you have to move a data point to increase the probability of it. And increasing the log probability means that you also increase the probability because the logarithm increases monotonically. So basically the true score function can tell you for any point in your data space in which direction you have to move to get closer to actual data points. The paper shows some intuitive visualizations. Given this data density the scores look like the following. The errors indicate the scores and you can see they always point towards the closest data center. Similarly in the case of images, let's say you are at a point you sample from a Gaussian distribution, inputting this into the score function will give you a direction you should move to get closer to the image manifold. So now we just need to minimize this objective and we can sample new data points. But unfortunately there is a catch. Our biggest problem is that we don't know gradient with respect to x log of p of x. Just like we don't know p of x. We also don't have access to the gradient log of it. However we can apply some math tricks to solve this problem. We'll start from the objective above. And we can expand the expectation as an integral by following the general rule of this. Let's read it out. Next we apply the algebraic identity which is what you probably learned in school already and get this. And now we separate the different parts into individual integrals. Let's take a closer look at the last term. We can apply another rule to simplify things later on. We can rewrite nabla x log p of x as nabla x p of x divided by p of x. Plugging this into the last term will look like the following. And now we can apply integration by parts to this integral. The formula for integration by parts is this. So if we have an integral, where we multiply the derivative or gradient of a function with another function, we can rewrite it with the formula on the right hand side. You can just look up integration by parts to understand the details. Applying this to our formula results in the following. The first term will become zero, because as x goes to infinity, probability densities typically decay to zero. A good example of this is the normal distribution. The further you go away from the mean, the closer the probability density gets to zero. Therefore we have this. Now we can plug this back into the previous equation. We can see that the first term does not involve the learned score model as theta, and therefore it just acts as a constant and for the optimization process we can ignore it. Now we can rewrite the integrals back as an expectation using the aforementioned formula of the expected value with respect to p of x is equal to the integral of p of x dx. So to summarize, we started at the formula that compares original scores with the predicted scores, realized that we don't have access to the original scores, and started reformulating and ended up at an equivalent objective without needing the original scores, which is the same up to a constant. But a constant does not change an optimization process. And this formula is very beautiful if you look at what the two terms mean. Remember that we want to minimize this objective. Looking at the second term, we see that we want the gradient of the score model at data points to be zero. This means it should be a local maximum. And the first term tries to make the predicted scores of the score model to be zero at data points. Remember that the score refers to the direction we have to go to increase likelihood for data points. And if the scores are zero, it means we're at a data point. Now unfortunately this gives us two problems. One is that the objective is very expensive to compute because of this term. This requires calculating the Jacobian, which does an individual back propagation for every input variable. This becomes a very big problem when the input space is very large, like it is for images, where there can be millions of pixels in the input. In the follow-up paper, Yang Song and colleagues propose a way to overcome this expensive training in a method called slice score matching. I won't go into the details here because the final method that we'll be arriving at won't make use of this idea. And as far as I know, the method is not used in the classical large-scale generative AI field. But I will link the paper if you want to read it. The second problem can be seen when looking at the example data densities. During training, we mostly see data points that come from the very likely areas of the data space. So data points would be close to this normal distribution or this. So our model becomes really good at learning the score function in these areas. However, the problem comes during sampling, when we start from a random position. This position could be anywhere here. It could be here or here or here. Our model has never seen these areas during training and might not know which direction the data densities lie. The paper shows this example how the true scores and the estimated scores look like. And you can clearly see that in many cases the scores point towards here, which lies in the middle of both data densities, but does not represent correct samples. Now one way of solving the second issue is by adding something that will bring us closer to something that looks like diffusion. We'll noise our data. The result of this will be that we cover more of the data space as you can see here. And we simply do this by taking all of our data points and adding a bit of Gaussian noise to them. Specifically, we'll take a data point and we'll just add epsilon, which comes from a normal distribution given the mean of zero and some standard deviation sigma. And we denote the new distribution as p sigma of x tilde. So if we train our model with the previously discussed original score matching algorithm with the noise perturbation, we solve the problem that when we start sampling from regions where there is little amount of data, our model has still seen those regions during training because we added noise to our data and thus the predicted scores are likely going to make sense. How much noise we add is dependent on a variable, which we will denote as sigma. We can add this to our training objective too. By the way, if you're very familiar with diffusion, you might get confused and think sigma is a varying value that is determined by a noise schedule. But this here is much simpler. Sigma is a single number that we choose as a hyperparameter. If we choose sigma to be large, we cover a lot of data space. However, this comes with the problem that our perturb distribution is very different from the original one. But if it's too small, the space we cover is much smaller as well. So we have a trade-off here. But this gives us a good way to approach the second problem that we mentioned earlier. Now we still have the problem that using the original objective is very expensive to compute. So we need to find a way to overcome this. Before that, let's briefly summarize. Our goal is to learn a model that given some point in our data space predicts the direction where we should move to get closer to data. Learning the scores is much easier because we don't have a normalizing constant that we need to approximate. So our objective looks quite easy. However, we don't have access to the original scores, which is why some clever people found the remarkable reformulation, which doesn't depend on having the original scores. Now we found two problems. One being that this is very expensive to calculate, and the second being the fact that our score model is not trained on inputs from the entire space. We can find a workaround for the latter by introducing noise perturbations, which unfortunately comes with a trade-off, which is that too much noise is not representative of our data anymore and too little noise is not covering a lot of this data space. Now there's still the first problem of the expensive computation. Luckily in 2010, a really smart person called Pascal Vincent saw a connection between score matching and denoising autoencoders. Denoising autoencoders were used back in the days to learn representative features of data. The idea was to train a regular autoencoder, but to add noise in the bottleneck and give that to the decoder, which is tasked to predict the original input. By being able to separate noise from data, it was possible that the model learned important features from the data. He looked at score matching and tried to introduce some concepts from denoising autoencoders. He started from the original objective with the noise perturbed data distributions. What follows now is quite a long derivation, to arrive at an objective that looks almost exactly like this one. I'm already showing this to you now, so that you know where we will arrive. You see that the only difference is basically that we don't have p sigma of x tilde anymore, but we have p of sigma of x tilde given x. As I'll explain later, this will make a huge difference, because we can evaluate the score of this directly while we can't calculate the scores of p sigma of x tilde. But first, let's do the derivation. The first part is very similar to what we did before. We'll take the objective and rewrite the expectation as the integral again. Note that this time we work with x tilde and not x anymore. Now we expand using the algebraic identity again and separate the different terms into their own integral. Let's move the 2 outside, which cancels with the 1 half. And now we will look at the last term again and do some fancy maths with it. First, we make use of this logarithm rule again, which allows us to write the gradient log of the perturbed noise distribution with this. Injecting this into the equation gives us the following. And here you see that p sigma of x tilde cancels and we are left with just this. Now let's make use of marginalization and write p sigma of x tilde as its marginal probability by integrating over x. In the previous reformulation we applied integration by parts instead of marginalization, because we couldn't do this before. And you'll see that this will allow us to simplify things in a little bit. Now we can plug this back into the formula, which gives us this. And now another math rule, which is called the Leibniz integration rule. So many math rules, huh? This one only states that under certain conditions the differentiation operator, in our case the gradient, can be moved inside of the integral. Since p of x doesn't depend on x tilde, we can move it directly to p sigma of x tilde given x. Now earlier we saw the following log rule. If you multiply this by f of x, we get the following equation. Now we can apply this reformulation to the gradient with respect to x tilde, p sigma of x tilde given x. And we get the following, which we can plug into the formula again. Also, know that if this is too fast, which it certainly is, just pause the video and take your time to understand it. Now we can apply the linearity of integrals and move s theta of x tilde into the inner integral, because it is only a constant for it. Now remember that this is the last term of our full equation that we are rewriting the whole time. Let's bring it back into the formula. And now let's rewrite the integrals back into expectations. Alright, we're almost done and we'll arrive at the final objective soon. To get there, we'll perform some operations on the last two terms. First, let's rewrite this term as this. The only thing that changes is that we rewrite where x tilde is drawn from, which we can do because of marginalization, which we discussed before. You may think, hmm, why do we do this? Well, now we're able to write the last two terms as one expectation. And notice that we have to multiply the second term by 2, because we have one half in front of the first term and the second term didn't have that. And now let's add this term, the gradient with respect to x tilde log p sigma of x tilde given x and subtracted immediately afterwards again. Useless, right? Well, now you can see that this whole part is equal to just this, which is the algebraic identity, just reversed. And as a quick side note, a lot of these steps probably required a lot of thinking for the people that came up with them and are not obvious at all, at least to me. So, don't worry if some of these steps seem super arbitrary. As long as you can understand the different steps and keep the overall goal in mind, you're good. I just show all these derivations, because a lot of the papers leave them away and I think actually going through them can help a lot with learning and understanding how you can approach such problems. And it's just satisfying. And now we can separate the last term from the mean squared error and now we have three different terms again. So now this is our full objective, which in theory we can try to minimize, optimizing the score model with its parameters theta. But the cool thing is that the first term and the last term don't include the score model, so they just become a constant in the objective and therefore we can ignore them. And this leaves us with the final objective. And this is exactly the objective I showed you in the beginning, which is super similar to the previous one. Now let me show you why this is so much better and why we can calculate the gradient with respect to x tilde log p sigma of x tilde given x. This is also where the connection to denoising autoencoders come in. There you encode your data, and noise it, and then give it to the decoder to reconstruct the original signal. And then you take the mean squared error between data and reconstruct it. Now x tilde, which is equal to x plus epsilon, corresponds to a conditional density of the following. Here x tilde is normally distributed around x, and we expressed it as the pdf of a multivariate Gaussian distribution. And now look at this. This is the same as what we have in our objective here. Now can we calculate this entire expression now? Well, let's try it. We can write out the expression like this. Now we can apply the log rule to separate this factor from the rest, which gives us this expression. Now the first term is zero, because there is no x tilde in the expression, which makes it a constant and taking the derivative of a constant is zero. We can also see that the log and the exponential cancel out, leaving us with the following expression. And now we take the derivative of this with respect to x tilde. We have to apply the chain rule here, which first calculates the outer derivative and multiplies it by the inner derivative. The outer derivative is the exponent minus 1 and move to the front, leaving us with minus 2 divided by 2 sigma squared, which is minus 1 divided by sigma squared. The inner derivative of x tilde minus x is just 1, and multiplying this doesn't change anything. This leaves us with this equation now, which we can further simplify to 1 divided by sigma squared times x minus x tilde. Because minus a minus b is equal to minus a plus b, which is equal to b minus a. So to conclude, the gradient log of p sigma of x tilde given x simplifies to 1 over sigma squared x minus x tilde. And remember, this is what we need in our objective function as the ground truth. And different than before, we can actually calculate this now. Calculating this is super easy and super efficient. We don't need to evaluate n separate backward passes as with the previous objective. And remember that the noise input x tilde is just the original plus random noise. To get a better idea what this objective actually means, let's plug that in. Pull in the minus and see that the 2x cancel and we are left with just negative epsilon scaled by a factor, which we can also write by plugging in the scaled negative epsilon. So now let's think about what this actually does. Our model takes in the noised input and is trying to predict the negative epsilon, right? Visualizing this shows this very nicely. If this is our data and we noise our data point, which could end up here. The model should take this point and predict the negative of the noise that we added. And this prediction, the negative of the noise, points exactly into the direction of our data manifold. So where the data likelihood goes. And remember, this is exactly what score matching is all about, right? Finding a model that predicts the scores. And the score refers to the direction you have to move to increase the likelihood of seeing data. So this all makes sense and is super intuitive, right? It took us a while to get here, but to me that is very beautiful to see. And if you are already familiar with diffusion models, this has some similarities with epsilon prediction, right? But we'll get to this later. Looking back at our two problems we had, we have solved one that made training super expensive and we have a trade-off for the low coverage of our data space, which we can tackle by increasing or decreasing sigma to cover more or less of the space. This trade-off solution is not optimal and later we'll find a much better solution for it, by just extending this idea. For now though, let's talk about how we can actually generate new data points. So far we only talked about training. And we do that by sampling. The essential idea is that we start at a random point in our data space and we give the point to our score model, which predicts the direction we should move to get closer to data. If we just do this once, there's a high chance that the prediction will not be too good, because just following the linear direction can overshoot the data easily. This is why it is much better to repeat the prediction several times and move only a little bit after every prediction. The formula for this is super simple. It means that at every step we take the current x tilde and subtract the noise prediction multiplied by alpha, which is a very small number like 0.001. To showcase this, let's look back at the toy example and assume we have trained a score model to estimate the scores of this data distribution. Let's initialize a lot of points and let's sample with the formula I just showed and they all collapse to the same point, which is the mean of our data set in this case. The reason is that the scores always show directions towards higher likelihood, so they will always point to the mean of the data because that's where we have the highest density of data. In a real world setting this probably would show in a different way, but still be very noticeable. But there's an easy fix with a complicated name that we can use to overcome this. It's called Longevian dynamics. And for our case, the change is super simple. We just add the following to the formula from above. Epsilon is random Gaussian noise and the square root of 2 alpha is a very small number. There's a whole derivation why it is square root of 2 alpha and not just alpha. But I'm not going into this here because it's not super related to score matching itself. I will link some resources for it if you want to look at it. And if we use this for sampling, we now get nice outputs that don't collapse and could actually come from our data distribution. Awesome. So in summary, we found a different objective than before, which is much cheaper to compute and makes a lot of intuitive sense. The model now learns to predict noise that we added to our data, scaled by some value. Also, we've seen how we're able to sample from a model that we trained, which results in outputs that could come from our data distribution. Now remember that we still have this trade-off that adding noise covers more of our data space, but also changes our data distribution. Also, if we choose sigma to be very low, then when we calculate x tilde, Epsilon has a very small standard deviation obviously, and thus only contains small values. When our model needs to predict Epsilon given x tilde, this becomes extremely difficult because x tilde barely contains any noise. And therefore our training will not work so well and has a high variance. So how can we solve this? The solution for it will be very easy and not math heavy at all. So where do we stand? If we use noise perturbation that add a lot of noise, we cover a lot of the data space, but change our data distribution a lot. If we add little noise, we don't cover a lot of the data space, but don't change our data too much. So what about the following? We do both, and everything in between. We train our model on high noise levels, and middle noise levels, and low noise levels. In practice, we simply vary sigma between some predefined range, like between 0.01 and 25. So the model will see data points all over the space. We can also condition our model on the noise level. It is just free extra information that could help the model with the task of predicting the scores. Our noising formula changes slightly and looks like this now. You see that the only thing that changed is the index at the sigma, indicating that it is not a fixed single value anymore, but a sequence of different values, low and high values. So now during training, our model sees most of the data space when using large sigma, but also sees the clear data distribution when sigma is small. So we have the best from both worlds. And then during sampling, we can start at a random point and at the highest noise scale, and slowly move towards a data distribution while also lowering the noise scale. But how many noise levels do we use? In this paper by Yang Song and Stefano Amon, which was the first paper to use multiple noise levels for score matching, they use 1000. And this might remind you of the fusion models too, where we also take data points and noise them with varying amounts. In a follow up paper they showed that when you would increase the number of noise levels to infinity, the noise perturbation becomes a stochastic process. Now the benefit of this is that it improves the quality of your samples, will make things a bit easier implementation wise, and can allow you to compute exact log likelihoods for data samples. And another super incredible thing is that this will give us a link to the diffusion literature. You will be able to understand how you can derive all the big diffusion papers like DDPM, DDAM, EDM, etc. And this is super cool, because there are so many details left out in these big papers that make it super hard to understand how they arrived at their solutions. But given everything we discussed in this video, you will have very solid preliminaries. A comparison would be that this video teaches you how to engineer cars in general, and papers like DDPM, DDAM, and EDM teach you the specifics to build a Ferrari, Porsche, and Tesla. But you don't have to do it. If you followed until here, you have a good understanding of the fundamental idea of score based models. And that is really cool. But for those of you interested, let's take a look. Letting the number of noise levels approach infinity, gives us a stochastic process. A stochastic process describes systems that evolve over time, which inherently have some randomness, like the stock market or climate models. And we can use, wait for it, fancy term, stochastic differential equations to model stochastic processes. The general form of a stochastic differential equation, or SDE, is the following. Dx refers to the change in x, which is described by the right hand side of this equation. F of x and t is called the drift coefficient, and g of t is called the diffusion coefficient. W stands for the Wiener process, which is named after Norbert Wiener, and basically just refers to normally distributed noise. And dw refers to the infinite small changes in the noise. The drift is fixed and basically describes if and how we change x in a deterministic way. Like for example, we scale it up or we scale it down. And the diffusion coefficient describes the influence of the stochasticity over time. For example, at earlier times, x may not experience a lot of change due to randomness, because g of t is small. But as we keep going and the diffusion coefficient increases, x could be changed much more by the noise. So with this formulation we have a way to determine how we want our data to change with time, given a deterministic influence and a stochastic influence. Now remember how we noise our data in score matching? It was the following. Now we know that as t increases, so does our noise level. So our change in x is only influenced by a stochastic term. In other words, there is no drift in our process and only a diffusion coefficient. Therefore the drift is zero and we only use the second term to model the stochastic process. In the limit, as noise scales approach infinity, it doesn't make so much sense anymore to use an index here and a function of t would make more sense. Like before, if we assume we had one thousand noise scales, we just create a list with the different sigmas and then we can just take from this list. As we have infinite noise scales, a list doesn't work anymore. But if we have a function that could take in any time step and return a corresponding sigma, that would be awesome. So let's simply call this function sigma of t. And let's assume t is a value between zero and one. Zero would mean that we add the least amount of noise and one would refer to the highest noise level. Now we didn't specify how we create this list of noise scales and also didn't define a function. We won't be using this specifically for the rest of the video, but in practice there are many ways that you could define this. The easiest would be to just set the smallest and highest sigma and then just do a linear interpolation between them. Other functions might work better or worse, but you get the idea. The field of differential equations is a well studied field and has been used for other applications long before score matching and diffusion models. What I've shown you here is a stochastic differential equation that corresponds to the process of noising our data. Let's call it the forward SDE. Analogously to the discrete case where we use Longevian dynamics to sample, here we can find an SDE for sampling. We will call this the reverse SDE. Now in 1982, another smart person called Brian Anderson came up with a general reverse SDE if we have a forward SDE that has this exact form. This general solution looks like the following. There's a whole paper by Anderson about how to derive the reverse SDE from the forward SDE. I will not cover it in this video here, since it would probably make it way longer, but you can find a link to the paper in the description. So we will just take this as a given. All you should take away from this for now is that we have a stochastic continuous process to noise our data and one that can reverse it. And you might have already seen something very familiar in this formula. This here is the score function we have been talking about all the time. So in order to reverse a noising process of our data, we can use the score function. And that is something we already knew, right? But still, it is nice to see that the solution to this very general problem of reversing a forward SDE involves the score function. Now given that we have a general form of a forward and reverse SDE, there are many specifications that we can do, some of which you may already know. Remember the way that we noise our data. In DDPM, you noise data like this, which includes interpolating between the original X and a noise drawn from a standard normal. So a stochastic process that models this noising behaviour is different than in our previous example, because the original X is also modified, whereas here, X stays the same and is dominated by a growing Epsilon. So a drift is not just zero, but actually determined. Now given the general formula of the forward and reverse SDE from before, we can convert the DDPM noising into the continuous case, which gives us the following for the forward SDE. Now getting here involves a bit of maths again. And for the people that are interested in this, I will show you the derivation here on screen and you can pause and follow the steps. But I won't actually talk through it. It goes a bit beyond the scope of this video, but in case someone is really interested, I thought it might be nice to have it here for completeness. And for the reverse SDE, you can make use of the general formula and plug in the different parts and you get your reverse SDE. And if you discretize this, you will actually get the DDPM sampler that they show in the paper. However there, it seems to have come out of nowhere and the paper doesn't really give an explanation about it. But now you can actually fully comprehend how we got there. To discretize an SDE, you can make use of something called the Euler-Mariyama method. For the sake of completeness, I'm showing this to you here again, but won't go into the details. You can pause the video again if you want to follow this. What matters is that you see that by following the core ideas of score matching, which to me seem very intuitive, we can arrive at the popular diffusion framework of DDPM, which seems much more mysterious if you read the original paper. And not only can we derive DDPM this way, we can do the same with DDAM and EDM. And even more, you probably have heard of these fancy samplers that people use for generating images with stable diffusion like DPM++2M-Karas. Well, all of these can be derived from the framework of differential equations and are ultimately just different ways of solving the general reverse SDE. So I hope I can convey how beneficial it can be to view diffusion models from this angle. And honestly to me, it was such a big relief after learning about this because I didn't feel I fully understood where all the diffusion ideas originated. And I hope you may feel the same about it. But anyways, there was a lot of maths and theory and I think we should do a summary that captures the essential ideas that we discussed in this video. So, if we have some data, then there is some underlying function that can represent and generate this data. We can call it P . Now, we would like to learn this function, such that we are able to generate new data that fits our existing dataset. We can try to learn and express this as a probability density function, which is non-negative and sums up to 1. This here is a way to model the two requirements for the PDF. Learning Pθ requires learning this normalization constant, which is extremely expensive to compute, because you need to integrate over the entire data space. To overcome this, we can apply two mathematical operators, the logarithm and the gradient. And we call the result the score. This gives you this, then this, and then this becomes zero, which leaves us with this. So, we get rid of the normalization constant, and we can try to learn as θ to predict the original scores. But then we realize that we don't have access to the original scores and try to solve this. We came up with one objective that is equivalent to this, but doesn't require access to the original scores. Then we realized that this is very expensive to compute because of this gradient operator here, which needs as many backward passes as input dimensions that we have. And we realized another issue. Being that training this way, we are limiting the score model to a small area of the data space it sees during training, which is mostly only high probability areas. But during sampling, we will most likely start in low probability areas. We can tackle the second problem by introducing noise perturbations and add noise to our data. This gives us a trade-off. Adding a lot of noise covers more of the data space, but changes our data function a lot. And adding just a bit of noise doesn't cover much, but leaves the original function intact. Given this, a connection to denoising autoencoders was found, which allows for something extremely cool. We can reformulate this objective to this. And this small difference in the formula makes a huge difference for us, because now we can calculate this term, which was not possible here. And by working this out, we get the following objective, which literally says that our score function should predict the negative of the noise that we added to x scaled by 1 divided by sigma squared. And that intuitively makes sense, because it's literally undoing the noising process. It's also easy to implement and cheap to compute. All of this solves this issue, but still leaves us with the sub-optimal trade-off to what extent we noise our data. And the solution for this was to use a varying range of noise levels during training. We sometimes noise a lot and sometimes noise only a bit. And with that our model learns to navigate the entire space that we could sample from. And this is very close to the typical way the fusion models are explained and used. But arriving at this objective through the score matching way gives a much clearer explanation in my opinion. Then we also saw that we can take this further and let the number of noise levels go to infinity and view the idea through stochastic processes, which showed us connections to the typical diffusion frameworks like DDPM and how to derive them. And this opens a lot of possibilities for you to dive deeper into diffusion and score-based models. You should now have a good starting point to read more recent and theoretical papers on these topics that go a lot more into the details. And hopefully the things I explained in this video will come in handy for you. So yeah, that was a very long video and I really hope you learned something and might enjoy this approach to diffusion models as much as I do. And if you're still watching this video that probably means you're very interested in this topic. So let me ask you, would you like to see more videos on diffusion models? Would you be interested in a whole series on the field of generative diffusion models? Let me know if that sounds interesting and also if there are other topics that you would like to see covered. So yeah, share the video to your friends. It was so much work and I'll hopefully see you in the next video.