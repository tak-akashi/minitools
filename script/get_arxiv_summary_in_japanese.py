#!/usr/bin/env python3
"""
arXivè«–æ–‡è¦ç´„ãƒ„ãƒ¼ãƒ«
arXivã‹ã‚‰æŒ‡å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è«–æ–‡ã‚’æ¤œç´¢ã—ã€ãã®è¦ç´„ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã€çµæœã‚’Slackã«é€šçŸ¥
ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚ŠåŠ¹ç‡åŒ–
"""

import os
import json
import feedparser
import ollama
import asyncio
import aiohttp
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timedelta
import pytz
import argparse
import logging
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

# ä¸¦åˆ—å‡¦ç†ã®è¨­å®š
MAX_CONCURRENT_PAPERS = 10    # åŒæ™‚ã«å‡¦ç†ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
MAX_CONCURRENT_OLLAMA = 5     # Ollama APIã¸ã®åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
MAX_CONCURRENT_NOTION = 3     # Notion APIã¸ã®åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°

def setup_logger() -> logging.Logger:
    """ãƒ­ã‚¬ãƒ¼ã®è¨­å®š"""
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©
    log_dir = Path("outputs/logs")
    log_dir.mkdir(parents=True, exist_ok=True)
    file_handler = logging.FileHandler(log_dir / "arxiv.log", mode="a")
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    return logger

logger = setup_logger()

NOTION_API_KEY = os.getenv("NOTION_API_KEY")
DATABASE_ID = os.getenv("NOTION_DB_ID")
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")


class ArxivProcessorAsync:
    """arXivè«–æ–‡ã‚’ä¸¦åˆ—å‡¦ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.ollama_client = ollama.Client()
        self.http_session = None
        self.ollama_semaphore = None
        self.notion_semaphore = None
        
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼"""
        connector = aiohttp.TCPConnector(limit=20)
        timeout = aiohttp.ClientTimeout(total=120)
        self.http_session = aiohttp.ClientSession(connector=connector, timeout=timeout)
        
        # ã‚»ãƒãƒ•ã‚©ã®åˆæœŸåŒ–
        self.ollama_semaphore = asyncio.Semaphore(MAX_CONCURRENT_OLLAMA)
        self.notion_semaphore = asyncio.Semaphore(MAX_CONCURRENT_NOTION)
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.http_session:
            await self.http_session.close()

    def search_arxiv(self, queries: List[str], start_date: str, end_date: str, max_results: int) -> List[Dict[str, str]]:
        """
        arXiv APIã‚’ä½¿ç”¨ã—ã¦è«–æ–‡ã‚’æ¤œç´¢
        
        Args:
            queries: æ¤œç´¢èªã®ãƒªã‚¹ãƒˆ
            start_date: æ¤œç´¢é–‹å§‹æ—¥ï¼ˆYYYYMMDDå½¢å¼ï¼‰
            end_date: æ¤œç´¢çµ‚äº†æ—¥ï¼ˆYYYYMMDDå½¢å¼ï¼‰
            max_results: å–å¾—ã™ã‚‹æœ€å¤§çµæœæ•°
        
        Returns:
            æ¤œç´¢çµæœã®è«–æ–‡æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        import requests  # åŒæœŸçš„ãªæ¤œç´¢å‡¦ç†
        
        url = "http://export.arxiv.org/api/query"
        search_query = " AND ".join(queries)
        
        params = {
            "search_query": f"all:{search_query} AND submittedDate:[{start_date}000000 TO {end_date}235959]",
            "start": 0,
            "max_results": max_results,
            "sortBy": "submittedDate",
            "sortOrder": "ascending",
        }
        
        response = requests.get(url, params=params)
        feed = feedparser.parse(response.content)
        
        papers = []
        for entry in feed.entries:
            title = entry.title.replace("\n", "")
            pdf_url = next((link.href for link in entry.links if link.get('title') == 'pdf'), None)
            
            if pdf_url:
                papers.append({
                    "title": title,
                    "updated_date": entry.updated,
                    "published_date": entry.published,
                    "summary": entry.summary,
                    "pdf_url": pdf_url
                })
        
        return papers

    async def translate_to_japanese_async(self, text: str, model: str = "gemma3:27b", retry_count: int = 3) -> str:
        """
        ollamaã‚’ä½¿ç”¨ã—ã¦æ—¥æœ¬èªã«ç¿»è¨³ï¼ˆéåŒæœŸç‰ˆãƒ»ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
        
        Args:
            text: ç¿»è¨³ã™ã‚‹è‹±èªã®ãƒ†ã‚­ã‚¹ãƒˆ
            model: ä½¿ç”¨ã™ã‚‹ollamaãƒ¢ãƒ‡ãƒ«
            retry_count: ãƒªãƒˆãƒ©ã‚¤å›æ•°
        
        Returns:
            æ—¥æœ¬èªã«ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        async with self.ollama_semaphore:
            for attempt in range(retry_count):
                try:
                    # Ollamaã¯åŒæœŸAPIãªã®ã§ã€executorã§å®Ÿè¡Œ
                    loop = asyncio.get_event_loop()
                    response = await loop.run_in_executor(
                        None,
                        lambda: self.ollama_client.chat(
                            model=model,
                            messages=[{
                                "role": "user",
                                "content": f"ä»¥ä¸‹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ã€‚\n\n{text}"
                            }]
                        )
                    )
                    return response["message"]["content"].strip()
                    
                except Exception as e:
                    if attempt < retry_count - 1:
                        wait_time = (attempt + 1) * 2  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                        logger.warning(f"ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}. {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"ç¿»è¨³ã‚¨ãƒ©ãƒ¼ (ãƒªãƒˆãƒ©ã‚¤å›æ•°è¶…é): {e}")
                        return "ç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ"

    async def summarize_paper_briefly_async(self, title: str, summary: str, model: str = "gemma3:27b") -> str:
        """
        è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‹ã‚‰ç°¡æ½”ãªè¦ç´„ã‚’ç”Ÿæˆï¼ˆéåŒæœŸç‰ˆï¼‰
        """
        async with self.ollama_semaphore:
            try:
                prompt = f"""ä»¥ä¸‹ã®è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‚’1ã€œ2æ–‡ã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚
        
ã‚¿ã‚¤ãƒˆãƒ«: {title}
è¦ç´„: {summary}

ç°¡æ½”ãªè¦ç´„:"""
                
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: self.ollama_client.chat(
                        model=model,
                        messages=[{"role": "user", "content": prompt}]
                    )
                )
                return response["message"]["content"].strip()
                
            except Exception as e:
                logger.error(f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                return f"{title}ã®è¦ç´„ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"

    async def add_to_notion_async(
        self,
        title: str,
        published_date: str,
        updated_date: str,
        summary: str,
        translated_summary: str,
        url: str,
        retry_count: int = 3
    ) -> bool:
        """
        Notion APIã«ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡ï¼ˆéåŒæœŸç‰ˆãƒ»ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
        
        Returns:
            ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆTrue
        """
        if not NOTION_API_KEY or not DATABASE_ID:
            logger.error("Notion API key or Database ID not set")
            return True
        
        async with self.notion_semaphore:
            for attempt in range(retry_count):
                try:
                    api_url = "https://api.notion.com/v1/pages"
                    headers = {
                        "Authorization": f"Bearer {NOTION_API_KEY}",
                        "Content-Type": "application/json",
                        "Notion-Version": "2022-06-28"
                    }
                    
                    data = {
                        "parent": {"database_id": DATABASE_ID},
                        "properties": {
                            "ã‚¿ã‚¤ãƒˆãƒ«": {
                                "title": [{"text": {"content": title}}]
                            },
                            "å…¬é–‹æ—¥": {
                                "date": {"start": published_date}
                            },
                            "æ›´æ–°æ—¥": {
                                "date": {"start": updated_date}
                            },
                            "æ¦‚è¦": {
                                "rich_text": [{"text": {"content": summary[:2000]}}]  # Notion API limit
                            },
                            "æ—¥æœ¬èªè¨³": {
                                "rich_text": [{"text": {"content": translated_summary[:2000]}}]
                            },
                            "URL": {
                                "url": url
                            }
                        }
                    }
                    
                    async with self.http_session.post(api_url, headers=headers, json=data) as response:
                        if response.status == 200:
                            logger.info(f"Added '{title}' to Notion.")
                            return False
                        else:
                            error_text = await response.text()
                            if attempt < retry_count - 1:
                                wait_time = (attempt + 1) * 2
                                logger.warning(f"Notion APIã‚¨ãƒ©ãƒ¼ (å†è©¦è¡Œ {attempt + 1}/{retry_count}): {response.status}. {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                                await asyncio.sleep(wait_time)
                            else:
                                logger.error(f"Failed to add to Notion. Status: {response.status}, Response: {error_text}")
                                return True
                                
                except Exception as e:
                    if attempt < retry_count - 1:
                        wait_time = (attempt + 1) * 2
                        logger.warning(f"Notionä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}. {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"Error adding to Notion: {e}")
                        return True
            
            return True

    async def process_paper_async(self, paper: Dict[str, str], index: int, total: int) -> Tuple[bool, Dict[str, str]]:
        """
        å˜ä¸€ã®è«–æ–‡ã‚’éåŒæœŸã§å‡¦ç†
        
        Returns:
            (ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‹ã©ã†ã‹, å‡¦ç†æ¸ˆã¿è«–æ–‡ãƒ‡ãƒ¼ã‚¿)
        """
        logger.info(f"Processing {paper['title']} ({index + 1}/{total})")
        
        try:
            # æ—¥æœ¬èªã«ç¿»è¨³
            translated_summary = await self.translate_to_japanese_async(paper["summary"])
            
            # Notionã«è¿½åŠ 
            error = await self.add_to_notion_async(
                paper['title'],
                paper["published_date"],
                paper["updated_date"],
                paper["summary"],
                translated_summary,
                paper['pdf_url']
            )
            
            # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            processed_paper = paper.copy()
            processed_paper['translated_summary'] = translated_summary
            
            return error, processed_paper
            
        except Exception as e:
            logger.error(f"è«–æ–‡å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({paper['title']}): {e}")
            processed_paper = paper.copy()
            processed_paper['translated_summary'] = "å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ"
            return True, processed_paper

    async def process_papers_async(self, papers: List[Dict[str, str]]) -> Tuple[int, List[Dict[str, str]]]:
        """
        è«–æ–‡ã‚’ä¸¦åˆ—å‡¦ç†ã—ã¦Notionã«ä¿å­˜
        
        Returns:
            (ã‚¨ãƒ©ãƒ¼æ•°, å‡¦ç†æ¸ˆã¿è«–æ–‡ãƒªã‚¹ãƒˆ)
        """
        if not papers:
            return 0, []
        
        error_count = 0
        processed_papers = []
        
        # è«–æ–‡ã‚’ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦å‡¦ç†
        for i in range(0, len(papers), MAX_CONCURRENT_PAPERS):
            batch = papers[i:i + MAX_CONCURRENT_PAPERS]
            batch_num = i // MAX_CONCURRENT_PAPERS + 1
            total_batches = (len(papers) + MAX_CONCURRENT_PAPERS - 1) // MAX_CONCURRENT_PAPERS
            
            logger.info(f"ãƒãƒƒãƒ {batch_num}/{total_batches} ã‚’å‡¦ç†ä¸­ï¼ˆ{len(batch)}ä»¶ï¼‰...")
            
            # ãƒãƒƒãƒå†…ã®è«–æ–‡ã‚’ä¸¦åˆ—å‡¦ç†
            tasks = [
                self.process_paper_async(paper, i + idx, len(papers))
                for idx, paper in enumerate(batch)
            ]
            
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # çµæœã‚’å‡¦ç†
            for idx, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    logger.error(f"è«–æ–‡å‡¦ç†ã‚¨ãƒ©ãƒ¼: {batch[idx]['title']} - {result}")
                    error_count += 1
                    # ã‚¨ãƒ©ãƒ¼ã§ã‚‚åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã¯ä¿å­˜
                    processed_paper = batch[idx].copy()
                    processed_paper['translated_summary'] = "å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ"
                    processed_papers.append(processed_paper)
                else:
                    error, processed_paper = result
                    if error:
                        error_count += 1
                    processed_papers.append(processed_paper)
        
        return error_count, processed_papers

    async def format_slack_message_async(self, papers: List[dict], date: str) -> str:
        """
        è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’SlackæŠ•ç¨¿ç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆéåŒæœŸç‰ˆï¼‰
        """
        if not papers:
            return f"*{date}ã®arXivè«–æ–‡*\næœ¬æ—¥ã¯å¯¾è±¡è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        message = f"*{date}ã®arXivè«–æ–‡ ({len(papers)}ä»¶)*\n\n"
        
        # å„è«–æ–‡ã®è¦ç´„ã‚’ä¸¦åˆ—ç”Ÿæˆ
        tasks = [
            self.summarize_paper_briefly_async(paper['title'], paper['summary'])
            for paper in papers
        ]
        
        brief_summaries = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, (paper, brief_summary) in enumerate(zip(papers, brief_summaries), 1):
            if isinstance(brief_summary, Exception):
                brief_summary = f"{paper['title']}ã®è¦ç´„ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
            
            message += f"{i}. *{paper['title']}*\n"
            message += f"   ğŸ“„ {brief_summary}\n"
            message += f"   ğŸ”— <{paper['pdf_url']}|PDF>\n\n"
        
        return message

    async def send_to_slack_async(self, message: str) -> bool:
        """
        Slackã«æŠ•ç¨¿ï¼ˆéåŒæœŸç‰ˆï¼‰
        """
        if not SLACK_WEBHOOK_URL:
            logger.error("SLACK_WEBHOOK_URL environment variable not set")
            return False
        
        payload = {"text": message}
        
        try:
            async with self.http_session.post(SLACK_WEBHOOK_URL, json=payload) as response:
                if response.status == 200:
                    logger.info("Successfully sent message to Slack")
                    return True
                else:
                    logger.error(f"Failed to send to Slack. Status code: {response.status}")
                    return False
        except Exception as e:
            logger.error(f"Error sending to Slack: {e}")
            return False

    async def main_async(
        self,
        queries: List[str],
        start_date: str,
        end_date: str,
        max_results: int,
        send_slack: bool = True
    ) -> None:
        """
        ãƒ¡ã‚¤ãƒ³å‡¦ç†
        """
        logger.info(
            f"Searching max {max_results} papers from {start_date} to {end_date} "
            f"with queries: {queries}"
        )
        
        # è«–æ–‡ã‚’æ¤œç´¢
        papers = self.search_arxiv(
            queries,
            start_date.replace("-", ""),
            end_date.replace("-", ""),
            max_results
        )
        logger.info(f"Found {len(papers)} papers")
        
        if not papers:
            logger.info("No papers found")
            if send_slack:
                slack_message = await self.format_slack_message_async([], end_date)
                await self.send_to_slack_async(slack_message)
            return
        
        # è«–æ–‡ã‚’ä¸¦åˆ—å‡¦ç†
        error_count, processed_papers = await self.process_papers_async(papers)
        
        logger.info(
            f"Processed {len(papers) - error_count} papers successfully. "
            f"{error_count} papers failed."
        )
        
        # Slackã«æŠ•ç¨¿
        if send_slack:
            slack_message = await self.format_slack_message_async(processed_papers, end_date)
            await self.send_to_slack_async(slack_message)


def parse_args() -> argparse.Namespace:
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æ"""
    parser = argparse.ArgumentParser(
        description="arXivã‹ã‚‰è«–æ–‡ã‚’æ¤œç´¢ã—ã€Notionã«ä¿å­˜ã—ã¦Slackã«é€šçŸ¥"
    )
    
    jst = pytz.timezone('Asia/Tokyo')
    yesterday = (datetime.now(jst) - timedelta(days=1)).strftime("%Y-%m-%d")
    
    parser.add_argument(
        '-q', '--queries',
        nargs='+',
        default=["LLM", "(RAG OR FINETUNING OR AGENT)"],
        help="æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆè¤‡æ•°æŒ‡å®šå¯ï¼‰"
    )
    parser.add_argument(
        '-d', '--days_before',
        type=int,
        default=1,
        help="ä½•æ—¥å‰ã‹ã‚‰æ¤œç´¢ã™ã‚‹ã‹"
    )
    parser.add_argument(
        '-b', '--base_date',
        type=str,
        default=yesterday,
        help="åŸºæº–æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰"
    )
    parser.add_argument(
        '-r', '--max_results',
        type=int,
        default=50,
        help="å–å¾—ã™ã‚‹æœ€å¤§è«–æ–‡æ•°"
    )
    parser.add_argument(
        '--no-slack',
        action='store_true',
        help="Slackã¸ã®æŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—"
    )
    
    return parser.parse_args()


async def main():
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    args = parse_args()
    
    start_date = (
        datetime.strptime(args.base_date, "%Y-%m-%d") - 
        timedelta(days=args.days_before - 1)
    ).strftime("%Y-%m-%d")
    
    async with ArxivProcessorAsync() as processor:
        await processor.main_async(
            queries=args.queries,
            start_date=start_date,
            end_date=args.base_date,
            max_results=args.max_results,
            send_slack=not args.no_slack
        )


if __name__ == "__main__":
    asyncio.run(main())