#!/usr/bin/env python3
"""
Medium Daily Digest to Notion
GmailçµŒç”±ã§Medium Daily Digestãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—ã—ã€è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡ºã—ã¦æ—¥æœ¬èªè¦ç´„ã¨å…±ã«Notionã«ä¿å­˜
ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚ŠåŠ¹ç‡åŒ–
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pickle
import base64
import re
import argparse
import asyncio
import aiohttp
import signal
import socket
from pathlib import Path
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
import json
from dataclasses import dataclass
from urllib.parse import urlparse
import pytz

from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import httplib2

import requests
from bs4 import BeautifulSoup
from notion_client import Client
import ollama
from dotenv import load_dotenv
from minitools.utils.logger import setup_logger

load_dotenv()

# ã‚¹ã‚³ãƒ¼ãƒ—è¨­å®š
SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']

# Slackè¨­å®š
SLACK_WEBHOOK_URL = os.getenv('SLACK_WEBHOOK_URL_MEDIUM_DAILY_DIGEST')

# ä¸¦åˆ—å‡¦ç†ã®è¨­å®š
MAX_CONCURRENT_ARTICLES = 10  # åŒæ™‚ã«å‡¦ç†ã™ã‚‹è¨˜äº‹ã®æœ€å¤§æ•°
MAX_CONCURRENT_OLLAMA = 3     # Ollama APIã¸ã®åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
MAX_CONCURRENT_NOTION = 3     # Notion APIã¸ã®åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
MAX_CONCURRENT_HTTP = 10      # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®åŒæ™‚æ¥ç¶šæ•°

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = setup_logger(
    name=__name__,
    log_file="medium_daily_digest.log"
)

@dataclass
class Article:
    """è¨˜äº‹æƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    title: str
    url: str
    author: str
    japanese_title: str = ""
    summary: str = ""
    japanese_summary: str = ""
    date_processed: str = ""


class MediumDigestProcessorAsync:
    """Medium Daily Digestãƒ¡ãƒ¼ãƒ«ã‚’å‡¦ç†ã—ã¦Notionã«ä¿å­˜ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆéåŒæœŸç‰ˆï¼‰"""
    
    def __init__(self):
        self.gmail_service = None
        self.notion_client = None
        self.ollama_client = None
        self.http_session = None
        self.ollama_semaphore = None
        self.notion_semaphore = None
        self.setup_clients()
    
    def setup_clients(self):
        """å„ç¨®APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–"""
        # Gmail API
        self.gmail_service = self._authenticate_gmail()
        if self.gmail_service is None:
            raise ValueError("Gmail APIã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # Notion API
        notion_token = os.getenv('NOTION_API_KEY')
        if not notion_token:
            raise ValueError("NOTION_API_KEY(ç’°å¢ƒå¤‰æ•°)ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        self.notion_client = Client(auth=notion_token)
        
        # Ollama Client
        self.ollama_client = ollama.Client()
        
        # ã‚»ãƒãƒ•ã‚©ã®åˆæœŸåŒ–
        self.ollama_semaphore = asyncio.Semaphore(MAX_CONCURRENT_OLLAMA)
        self.notion_semaphore = asyncio.Semaphore(MAX_CONCURRENT_NOTION)
    
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼"""
        connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT_HTTP)
        timeout = aiohttp.ClientTimeout(total=60, connect=30, sock_connect=30, sock_read=30)
        self.http_session = aiohttp.ClientSession(connector=connector, timeout=timeout)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.http_session:
            await self.http_session.close()
    
    def _authenticate_gmail(self):
        """Gmail APIã®èªè¨¼"""
        try:
            creds = None
            token_path = 'token.pickle'
            
            if os.path.exists(token_path):
                with open(token_path, 'rb') as token:
                    creds = pickle.load(token)
            
            if not creds or not creds.valid:
                if creds and creds.expired and creds.refresh_token:
                    creds.refresh(Request())
                else:
                    credentials_path = os.getenv('GMAIL_CREDENTIALS_PATH', 'credentials.json')
                    if not os.path.exists(credentials_path):
                        raise FileNotFoundError(f"èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ« {credentials_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    
                    flow = InstalledAppFlow.from_client_secrets_file(credentials_path, SCOPES)
                    creds = flow.run_local_server(port=0)
                
                with open(token_path, 'wb') as token:
                    pickle.dump(creds, token)
            
            return build('gmail', 'v1', credentials=creds)
        except Exception as e:
            logger.error(f"Gmailèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def get_medium_digest_emails_async(self, date: Optional[datetime] = None) -> List[Dict]:
        """Medium Daily Digestãƒ¡ãƒ¼ãƒ«ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰å–å¾—"""
        if date is None:
            date = datetime.now()
        
        # JSTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’è¨­å®š
        jst = pytz.timezone('Asia/Tokyo')
        
        # dateãŒnaiveã®å ´åˆã¯JSTã¨ã—ã¦æ‰±ã†
        if date.tzinfo is None:
            date = jst.localize(date)
        
        # æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã®é–‹å§‹ã¨çµ‚äº†ã‚’è¨ˆç®—ï¼ˆJSTï¼‰
        start_date_jst = date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date_jst = start_date_jst + timedelta(days=1)
        
        # Unix ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›ï¼ˆã“ã‚Œã«ã‚ˆã‚Šã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã®å•é¡Œã‚’å›é¿ï¼‰
        start_timestamp = int(start_date_jst.timestamp())
        end_timestamp = int(end_date_jst.timestamp())
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
        logger.info(f"æ¤œç´¢æœŸé–“ (JST): {start_date_jst} ã‹ã‚‰ {end_date_jst}")
        logger.info(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {start_timestamp} ã‹ã‚‰ {end_timestamp}")
        
        # å·®å‡ºäººã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç¯„å›²ã§æ¤œç´¢
        query = f'from:noreply@medium.com after:{start_timestamp} before:{end_timestamp}'
        logger.info(f"Gmailæ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")
        
        try:
            # Gmail APIã®å‘¼ã³å‡ºã—ã‚’éåŒæœŸã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            
            # DNSè§£æ±ºã‚¨ãƒ©ãƒ¼ã®è©³ç´°æƒ…å ±ã‚’è¿½åŠ 
            try:
                response = await loop.run_in_executor(
                    None,
                    lambda: self.gmail_service.users().threads().list(
                        userId='me',
                        q=query,
                        maxResults=1
                    ).execute()
                )
            except socket.gaierror as e:
                logger.error(f"DNSè§£æ±ºã‚¨ãƒ©ãƒ¼: {e}")
                logger.error("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                raise
            except Exception as e:
                logger.error(f"Gmail APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
                raise

            threads = response.get('threads', [])
            if not threads:
                return []

            thread_id = threads[0]['id']
            thread = await loop.run_in_executor(
                None,
                lambda: self.gmail_service.users().threads().get(
                    userId='me',
                    id=thread_id
                ).execute()
            )
            
            # ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã®æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
            messages = thread.get('messages', [])
            if messages:
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å—ä¿¡æ™‚åˆ»ã‚‚ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                msg = messages[-1]
                if 'internalDate' in msg:
                    received_timestamp = int(msg['internalDate']) / 1000
                    received_date = datetime.fromtimestamp(received_timestamp, tz=jst)
                    logger.info(f"ãƒ¡ãƒ¼ãƒ«å—ä¿¡æ—¥æ™‚ (JST): {received_date}")
                
                return [messages[-1]] # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ãƒªã‚¹ãƒˆã§è¿”ã™
            return []

        except HttpError as error:
            logger.error(f'Gmail APIã‚¨ãƒ©ãƒ¼: {error}')
            return []
    
    async def get_email_content_async(self, message_id: str) -> str:
        """ãƒ¡ãƒ¼ãƒ«ã®æœ¬æ–‡ã‚’å–å¾—"""
        try:
            # Gmail APIã®å‘¼ã³å‡ºã—ã‚’éåŒæœŸã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            message = await loop.run_in_executor(
                None,
                lambda: self.gmail_service.users().messages().get(
                    userId='me',
                    id=message_id
                ).execute()
            )
            
            payload = message['payload']
            body = self._extract_body_from_payload(payload)
            
            return body
        
        except HttpError as error:
            logger.error(f'ãƒ¡ãƒ¼ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {error}')
            return ""
    
    def _extract_body_from_payload(self, payload: Dict) -> str:
        """ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‹ã‚‰ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‚’æŠ½å‡º"""
        body = ""
        
        if 'parts' in payload:
            for part in payload['parts']:
                if part['mimeType'] == 'text/html':
                    data = part['body']['data']
                    body = base64.urlsafe_b64decode(data).decode('utf-8')
                    break
                elif 'parts' in part:
                    body = self._extract_body_from_payload(part)
                    if body:
                        break
        elif payload['body'].get('data'):
            body = base64.urlsafe_b64decode(payload['body']['data']).decode('utf-8')
        
        return body
    
    def parse_articles_from_email(self, html_content: str) -> List[Article]:
        """ãƒ¡ãƒ¼ãƒ«HTMLã‹ã‚‰è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html_content, 'html.parser')
        articles = []
      
        # Medium Daily Digestã®è¨˜äº‹ãƒªãƒ³ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ï¼ˆclass="ag"ã®aã‚¿ã‚°ã®ã¿ï¼‰
        article_links = soup.find_all('a', class_='ag', href=re.compile(r'https://medium\.com/.*\?source=email'))
        
        seen_urls = set()
        
        for link in article_links:
            url = link.get('href', '')
            if not url or url in seen_urls:
                continue
            
            # URLã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®Œå…¨ã«é™¤å»ï¼‰
            clean_url = self._clean_url(url)
            if clean_url in seen_urls:
                logger.debug(f"é‡è¤‡URLã‚’ã‚¹ã‚­ãƒƒãƒ—: {clean_url}")
                continue
            seen_urls.add(clean_url)
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨è‘—è€…ã®æŠ½å‡º
            title = link.get_text(strip=True)
            if not title or len(title) < 10:
                logger.debug(f"çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {title}")
                continue
            
            # è‘—è€…æƒ…å ±ã®æŠ½å‡ºï¼ˆãƒªãƒ³ã‚¯ã®è¿‘ãã«ã‚ã‚‹å ´åˆãŒå¤šã„ï¼‰
            author = "Unknown"
            parent = link.parent
            if parent:
                author_text = parent.get_text()
                author_match = re.search(r'by\s+([^â€¢\n]+)', author_text)
                if author_match:
                    author = author_match.group(1).strip()
            
            article = Article(
                title=title,
                url=clean_url,
                author=author,
                date_processed=datetime.now().isoformat()
            )
            articles.append(article)
            logger.debug(f"è¨˜äº‹ã‚’æ¤œå‡º: {title[:50]}... by {author}")
        
        logger.info(f"åˆè¨ˆ{len(articles)}ä»¶ã®è¨˜äº‹ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        return articles
    
    def _clean_url(self, url: str) -> str:
        """URLã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é™¤å»ï¼‰"""
        # URLã‹ã‚‰ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
        clean_url = url.split('?')[0]
        # æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»
        clean_url = clean_url.rstrip('/')
        # Mediumã®ç‰¹æ®Šãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
        if '#' in clean_url:
            clean_url = clean_url.split('#')[0]
        return clean_url
    
    async def fetch_article_content_async(self, url: str, retry_count: int = 3) -> tuple[str, Optional[str]]:
        """è¨˜äº‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’éåŒæœŸã§å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
        for attempt in range(retry_count):
            try:
                async with self.http_session.get(url) as response:
                    response.raise_for_status()
                    content = await response.text()
                    
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # ã‚ˆã‚Šæ­£ç¢ºãªè‘—è€…åã‚’å–å¾—
                    author_tag = soup.find('a', attrs={'data-testid': 'authorName'})
                    author = author_tag.get_text(strip=True) if author_tag else None
                    
                    # è¨˜äº‹æœ¬æ–‡ã®æŠ½å‡º
                    article_body = soup.find('article')
                    if not article_body:
                        article_body = soup.find('div', class_='postArticle-content')
                    
                    text_content = article_body.get_text(separator=' ', strip=True)[:3000] if article_body else soup.get_text(separator=' ', strip=True)[:3000]
                    
                    return text_content, author
                    
            except aiohttp.ClientError as e:
                if attempt < retry_count - 1:
                    wait_time = (attempt + 1) * 2  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    logger.warning(f"è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}. {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}. ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¶…ãˆã¾ã—ãŸ")
                    return "", None
            except Exception as e:
                logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                return "", None
    
    async def generate_japanese_translation_and_summary_async(self, article: Article):
        """è¨˜äº‹ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ç¿»è¨³ã¨è¦ç´„ã‚’éåŒæœŸã§ç”Ÿæˆ"""
        try:
            logger.debug(f"è¨˜äº‹å‡¦ç†é–‹å§‹: {article.title[:50]}... ({article.url})")
            
            # è¨˜äº‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’éåŒæœŸã§å–å¾—
            text_content, author = await self.fetch_article_content_async(article.url)
            
            if author:
                article.author = author
            
            if not text_content:
                logger.warning(f"è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—å¤±æ•—: {article.title[:50]}...")
                article.japanese_title = "å–å¾—å¤±æ•—"
                article.japanese_summary = "è¨˜äº‹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
                return
            
            # Ollama APIã§ã‚¿ã‚¤ãƒˆãƒ«ç¿»è¨³ã¨è¦ç´„ç”Ÿæˆï¼ˆã‚»ãƒãƒ•ã‚©ã§åˆ¶é™ï¼‰
            async with self.ollama_semaphore:
                prompt = f"""You are a professional translator and summarizer. Respond in JSON format.

Translate the following article title to Japanese and summarize the article text in Japanese (around 200 characters).

Original Title: {article.title}
Author: {article.author}

Article Text:
{text_content}

---

Respond with a JSON object with two keys: 'japanese_title' and 'japanese_summary'.
Example:
{{
  "japanese_title": "ã“ã“ã«æ—¥æœ¬èªã®ã‚¿ã‚¤ãƒˆãƒ«",
  "japanese_summary": "ã“ã“ã«æ—¥æœ¬èªã®è¦ç´„"
}}
"""
                
                # Ollamaã¯åŒæœŸAPIãªã®ã§ã€executor ã§å®Ÿè¡Œ
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: self.ollama_client.chat(
                        model="gemma3:27b",
                        messages=[{"role": "user", "content": prompt}],
                        format="json"
                    )
                )
                
                # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
                result = json.loads(response.message.content)
                article.japanese_title = result.get('japanese_title', "ç¿»è¨³å¤±æ•—")
                article.japanese_summary = result.get('japanese_summary', "è¦ç´„å¤±æ•—")
                logger.debug(f"ç¿»è¨³ãƒ»è¦ç´„å®Œäº†: {article.japanese_title[:30]}...")

        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æã‚¨ãƒ©ãƒ¼ ({article.title[:50]}...): {e}")
            article.japanese_title = "ç¿»è¨³å¤±æ•—ï¼ˆJSONè§£æã‚¨ãƒ©ãƒ¼ï¼‰"
            article.japanese_summary = "è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
        except Exception as e:
            logger.error(f"ç¿»è¨³ãƒ»è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({article.title[:50]}...): {e}")
            article.japanese_title = "ç¿»è¨³å¤±æ•—"
            article.japanese_summary = "è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
    
    def format_slack_message(self, articles: List[Article], date: str) -> str:
        """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’SlackæŠ•ç¨¿ç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not articles:
            return f"*{date}ã®Medium Daily Digest*\næœ¬æ—¥ã¯å¯¾è±¡è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        message = f"*{date}ã®Medium Daily Digest ({len(articles)}ä»¶)*\n\n"
        
        for i, article in enumerate(articles, 1):
            message += f"{i}. *{article.japanese_title or article.title}*\n"
            message += f"   ğŸ‘¤ {article.author}\n"
            message += f"   ğŸ“„ {article.japanese_summary}\n"
            message += f"   ğŸ”— <{article.url}|è¨˜äº‹ã‚’èª­ã‚€>\n\n"
        
        return message
    
    async def send_to_slack_async(self, message: str) -> bool:
        """Slackã«éåŒæœŸã§æŠ•ç¨¿"""
        if not SLACK_WEBHOOK_URL:
            logger.error("SLACK_WEBHOOK_URLç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        payload = {"text": message}
        
        try:
            async with self.http_session.post(SLACK_WEBHOOK_URL, json=payload) as response:
                if response.status == 200:
                    logger.info("Slackã¸ã®é€ä¿¡ãŒå®Œäº†ã—ã¾ã—ãŸ")
                    return True
                else:
                    logger.error(f"Slackã¸ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status}")
                    return False
        except Exception as e:
            logger.error(f"Slackã¸ã®é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def save_to_notion_async(self, article: Article, target_date: datetime, retry_count: int = 3) -> str:
        """è¨˜äº‹æƒ…å ±ã‚’Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«éåŒæœŸã§ä¿å­˜ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
        
        Returns:
            'success': æˆåŠŸ
            'skipped': æ—¢å­˜ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
            'failed': å¤±æ•—
        """
        database_id = os.getenv('NOTION_DB_ID_DAILY_DIGEST')
        if not database_id:
            raise ValueError("NOTION_DATABASE_IDç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        async with self.notion_semaphore:
            for attempt in range(retry_count):
                try:
                    # Notion APIã¯åŒæœŸAPIãªã®ã§ã€executor ã§å®Ÿè¡Œ
                    loop = asyncio.get_event_loop()
                    
                    # æ—¢å­˜ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯
                    existing = await loop.run_in_executor(
                        None,
                        lambda: self.notion_client.databases.query(
                            database_id=database_id,
                            filter={
                                "property": "URL",
                                "url": {
                                    "equals": article.url
                                }
                            }
                        )
                    )

                    if existing['results']:
                        # æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã—ã¦ãƒ­ã‚°å‡ºåŠ›
                        existing_entry = existing['results'][0]
                        existing_date = existing_entry.get('properties', {}).get('Date', {}).get('date', {}).get('start', 'Unknown')
                        logger.info(f"  -> æ—¢ã«å­˜åœ¨ã™ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {article.title[:50]}... (æ—¢å­˜æ—¥ä»˜: {existing_date})")
                        return 'skipped'

                    # æ–°è¦ãƒšãƒ¼ã‚¸ä½œæˆ
                    logger.info(f"  -> Notionã«ä¿å­˜ä¸­: {article.title[:50]}...")
                    await loop.run_in_executor(
                        None,
                        lambda: self.notion_client.pages.create(
                            parent={"database_id": database_id},
                            properties={
                                "Title": {
                                    "title": [
                                        {
                                            "text": {
                                                "content": article.title
                                            }
                                        }
                                    ]
                                },
                                "Japanese Title": { 
                                    "rich_text": [
                                        {
                                            "text": {
                                                "content": article.japanese_title
                                            }
                                        }
                                    ]
                                },
                                           
                                "URL": {
                                    "url": article.url
                                },
                                "Author": {
                                    "rich_text": [
                                        {
                                            "text": {
                                                "content": article.author
                                            }
                                        }
                                    ]
                                },
                                "Summary": {
                                    "rich_text": [
                                        {
                                            "text": {
                                                "content": article.japanese_summary
                                            }
                                        }
                                    ]
                                },                             
                                "Date": {
                                    "date": {
                                        "start": target_date.strftime("%Y-%m-%d")
                                    }
                                }
                            }
                        )
                    )

                    logger.info(f"  -> ä¿å­˜å®Œäº†: {article.title[:50]}... by {article.author}")
                    return 'success'  # æˆåŠŸã—ãŸã‚‰çµ‚äº†

                except Exception as e:
                    if attempt < retry_count - 1:
                        wait_time = (attempt + 1) * 2  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                        logger.warning(f"  -> Notionä¿å­˜ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{retry_count}): {article.title[:50]}...")
                        logger.warning(f"     ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
                        logger.warning(f"     {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"  -> Notionä¿å­˜å¤±æ•— (å…¨{retry_count}å›è©¦è¡Œ): {article.title[:50]}...")
                        logger.error(f"     æœ€çµ‚ã‚¨ãƒ©ãƒ¼: {e}")
                        return 'failed'
    
    async def process_article_async(self, article: Article, target_date: datetime, save_notion: bool) -> tuple[Article, str]:
        """å˜ä¸€ã®è¨˜äº‹ã‚’éåŒæœŸã§å‡¦ç†
        
        Returns:
            (Article, status): è¨˜äº‹ã¨Notionã¸ã®ä¿å­˜ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹('success'/'skipped'/'failed'/None)
        """
        # ç¿»è¨³ã¨è¦ç´„ã‚’ç”Ÿæˆ
        await self.generate_japanese_translation_and_summary_async(article)
        
        # Notionã«ä¿å­˜ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        status = None
        if save_notion:
            status = await self.save_to_notion_async(article, target_date)
        
        return article, status
    
    async def process_daily_digest_async(self, target_date: datetime, save_notion: bool = True, send_slack: bool = True):
        """ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã®å‡¦ç†ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
        date_str = target_date.strftime('%Y-%m-%d')
        logger.info(f"Medium Daily Digest ({date_str}) ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")

        # Slacké€ä¿¡ãŒæœ‰åŠ¹ãªå ´åˆã€ç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯
        if send_slack and not SLACK_WEBHOOK_URL:
            raise ValueError("SLACK_WEBHOOK_URL_MEDIUM_DAILY_DIGESTç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Slacké€ä¿¡ã‚’è¡Œã†å ´åˆã¯è¨­å®šã—ã¦ãã ã•ã„ã€‚")

        # æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã®ãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—
        messages = await self.get_medium_digest_emails_async(target_date)

        if not messages:
            logger.warning(f"{date_str} ã®Daily Digestãƒ¡ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        # ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‚’å–å¾—
        message_id = messages[0]['id']
        email_content = await self.get_email_content_async(message_id)

        if not email_content:
            logger.error("ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

        # è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
        articles = self.parse_articles_from_email(email_content)
        logger.info(f"{len(articles)}ä»¶ã®è¨˜äº‹ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")

        if not articles:
            return

        # è¨˜äº‹ã‚’ä¸¦åˆ—å‡¦ç†ã§å‡¦ç†
        logger.info(f"è¨˜äº‹ã®ç¿»è¨³ã¨è¦ç´„ã‚’é–‹å§‹ã—ã¾ã™...")
        logger.info(f"æœ€å¤§{MAX_CONCURRENT_ARTICLES}ä»¶ã®è¨˜äº‹ã‚’ä¸¦åˆ—å‡¦ç†ã—ã¾ã™")
        
        # çµ±è¨ˆã®åˆæœŸåŒ–
        stats = {"success": 0, "skipped": 0, "failed": 0}
        
        # è¨˜äº‹ã‚’ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦å‡¦ç†
        processed_articles = []
        total_batches = (len(articles) + MAX_CONCURRENT_ARTICLES - 1) // MAX_CONCURRENT_ARTICLES
        
        for i in range(0, len(articles), MAX_CONCURRENT_ARTICLES):
            batch = articles[i:i + MAX_CONCURRENT_ARTICLES]
            batch_num = i // MAX_CONCURRENT_ARTICLES + 1
            logger.info(f"ãƒãƒƒãƒ {batch_num}/{total_batches} ã‚’å‡¦ç†ä¸­ ({len(batch)}ä»¶)...")
            
            # ãƒãƒƒãƒå†…ã®è¨˜äº‹ã‚’ä¸¦åˆ—å‡¦ç†
            tasks = [
                self.process_article_async(article, target_date, save_notion)
                for article in batch
            ]
            
            # ã‚¿ã‚¹ã‚¯ã‚’è¿½è·¡
            for task in tasks:
                _running_tasks.add(task)
            
            try:
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            finally:
                # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’å‰Šé™¤
                for task in tasks:
                    _running_tasks.discard(task)
            
            # ã‚¨ãƒ©ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æˆåŠŸã—ãŸè¨˜äº‹ã®ã¿ã‚’è¿½åŠ 
            for idx, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    logger.error(f"è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {batch[idx].title} - {result}")
                    if save_notion:
                        stats["failed"] += 1
                else:
                    article, status = result
                    processed_articles.append(article)
                    # çµ±è¨ˆã‚’æ›´æ–°
                    if status == 'success':
                        stats["success"] += 1
                    elif status == 'skipped':
                        stats["skipped"] += 1
                    elif status == 'failed':
                        stats["failed"] += 1

        # å‡¦ç†çµæœã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
        if save_notion:
            logger.info("=" * 60)
            logger.info(f"Notionã¸ã®ä¿å­˜çµæœ:")
            logger.info(f"  æˆåŠŸ: {stats['success']}ä»¶")
            logger.info(f"  ã‚¹ã‚­ãƒƒãƒ— (æ—¢å­˜): {stats['skipped']}ä»¶")
            logger.info(f"  å¤±æ•—: {stats['failed']}ä»¶")
            logger.info("=" * 60)
        
        # Slackã«é€ä¿¡
        if send_slack and processed_articles:
            slack_message = self.format_slack_message(processed_articles, date_str)
            await self.send_to_slack_async(slack_message)

        logger.info(f"å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"  å‡¦ç†è¨˜äº‹æ•°: {len(processed_articles)}/{len(articles)}ä»¶")
        logger.info(f"  å¯¾è±¡æ—¥ä»˜: {date_str}")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ã‚¿ã‚¹ã‚¯ã‚’ç®¡ç†
_running_tasks = set()

def signal_handler(signum, frame):
    """ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆCtrl+Cå¯¾å¿œï¼‰"""
    logger.info("\nå‡¦ç†ã‚’ä¸­æ–­ã—ã¦ã„ã¾ã™...")
    # å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
    for task in _running_tasks:
        task.cancel()
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’åœæ­¢
    loop = asyncio.get_event_loop()
    loop.stop()

async def main_async():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description='Medium Daily Digestãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—ã—ã¦Notionã«ä¿å­˜ãŠã‚ˆã³Slackã«é€ä¿¡')
    parser.add_argument(
        '--date',
        type=str,
        help='å–å¾—ã™ã‚‹æ—¥ä»˜ (YYYY-MM-DDå½¢å¼)ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ä»Šæ—¥ã®æ—¥ä»˜ã‚’ä½¿ç”¨',
        default=None
    )
    
    # ç›¸äº’æ’ä»–çš„ãªã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
    exclusive_group = parser.add_mutually_exclusive_group()
    exclusive_group.add_argument(
        '--slack',
        action='store_true',
        help='Slackã¸ã®é€ä¿¡ã®ã¿å®Ÿè¡Œï¼ˆNotionä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰',
        default=False
    )
    exclusive_group.add_argument(
        '--notion',
        action='store_true',
        help='Notionã¸ã®ä¿å­˜ã®ã¿å®Ÿè¡Œï¼ˆSlacké€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰',
        default=False
    )
    
    args = parser.parse_args()
    
    # æ—¥ä»˜ã®ãƒ‘ãƒ¼ã‚¹ã¨æ¤œè¨¼
    if args.date:
        try:
            target_date = datetime.strptime(args.date, '%Y-%m-%d')
        except ValueError:
            logger.error(f"ã‚¨ãƒ©ãƒ¼: æ—¥ä»˜ã¯ YYYY-MM-DD å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ (ä¾‹: 2024-01-15)")
            return
    else:
        target_date = datetime.now()
    
    # ãƒ•ãƒ©ã‚°ã«åŸºã¥ã„ã¦å‹•ä½œã‚’æ±ºå®š
    if args.slack:
        # --slackãƒ•ãƒ©ã‚°ãŒã‚ã‚‹å ´åˆï¼šSlacké€ä¿¡ã®ã¿
        save_notion = False
        send_slack = True
    elif args.notion:
        # --notionãƒ•ãƒ©ã‚°ãŒã‚ã‚‹å ´åˆï¼šNotionä¿å­˜ã®ã¿
        save_notion = True
        send_slack = False
    else:
        # ãƒ•ãƒ©ã‚°ãªã—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ï¼šä¸¡æ–¹å®Ÿè¡Œ
        save_notion = True
        send_slack = True
    
    try:
        async with MediumDigestProcessorAsync() as processor:
            await processor.process_daily_digest_async(target_date, save_notion=save_notion, send_slack=send_slack)
    except asyncio.CancelledError:
        logger.info("å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
        raise
    except socket.gaierror as e:
        logger.error(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        logger.error("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ã€å†åº¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        logger.error("DNSè¨­å®šã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        raise
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error(f"ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {type(e).__name__}")
        import traceback
        logger.error(f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}")
        raise


def main():
    """åŒæœŸçš„ãªã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        asyncio.run(main_async())
    except KeyboardInterrupt:
        logger.info("\nå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        raise


if __name__ == "__main__":
    main()